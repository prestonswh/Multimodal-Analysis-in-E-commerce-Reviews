{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files have been successfully merged and saved to 'merged_file.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "join_csv_2.py로 자동화 완료\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Load the first CSV file - CGI 포함된 데이터\n",
    "csv1 = pd.read_csv('/Users/ojeongsig/Desktop/yeonguno/did_test/06_14.csv')\n",
    "\n",
    "# Load the second CSV file - 초기 원본데이터\n",
    "csv2 = pd.read_csv('/Users/ojeongsig/Desktop/yeonguno/did_test/df_revie_backup.csv')\n",
    "\n",
    "# Set 'Unnamed: 0' as the index for the first DataFrame\n",
    "csv1.set_index('Unnamed: 0', inplace=True)\n",
    "\n",
    "# Select the specific columns to merge from csv1\n",
    "columns_to_merge = ['image_similarity', 'text_similarity', 'pair_similarity']\n",
    "\n",
    "# Merge the specified columns into csv2 based on the 'Unnamed: 0' index\n",
    "csv2 = csv2.merge(csv1[columns_to_merge], how='left', left_index=True, right_index=True)\n",
    "\n",
    "# Function to fill values only for rows below the first occurrence of non-missing values\n",
    "def fill_below_first_non_na(df, columns):\n",
    "    for col in columns:\n",
    "        mask = df[col].notna()\n",
    "        first_non_na_idx = mask.idxmax() if mask.any() else None\n",
    "        if first_non_na_idx:\n",
    "            df.loc[first_non_na_idx+1:, col] = df.loc[first_non_na_idx, col]\n",
    "    return df\n",
    "\n",
    "# Apply the function to each group of 'product_id'\n",
    "csv2 = csv2.groupby('product_id', group_keys=False).apply(fill_below_first_non_na, columns=columns_to_merge)\n",
    "\n",
    "# Fill NaN values with 0\n",
    "csv2[columns_to_merge] = csv2[columns_to_merge].fillna(0)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "csv2.to_csv('resnet_50_06_14.csv', index=False)\n",
    "\n",
    "print(\"The files have been successfully merged and saved to 'merged_file.csv'.\")\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found.\n",
      "Data processing complete. Files saved.\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    " \n",
    "input_file = '/Users/ojeongsig/Desktop/yeonguno/did_test/resnet_50_06_14'\n",
    "input_format = '.csv'\n",
    "#/Users/ojeongsig/Desktop/yeonguno/did_test/merged_file_processed.csv\n",
    "# Function to load data\n",
    "def load_data(filename):\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "# Function to process data\n",
    "def process_data(df):\n",
    "    # Identify duplicates based on product_id and review_date\n",
    "    duplicates = df[df.duplicated(subset=['product_id', 'review_date'], keep=False)]\n",
    "\n",
    "    # Print duplicates if they exist\n",
    "    if not duplicates.empty:\n",
    "        print(\"Duplicates found:\")\n",
    "        print(duplicates)\n",
    "    else:\n",
    "        print(\"No duplicates found.\")\n",
    "\n",
    "    # Drop duplicates based on product_id and review_date\n",
    "    df = df.drop_duplicates(subset=['product_id', 'review_date'])\n",
    "    \n",
    "    # Convert review_date to the specified datetime format\n",
    "    df['datetime'] = pd.to_datetime(df['review_date']).dt.strftime('%d%b%Y %H:%M:%S')\n",
    "    \n",
    "    # Extract month and year from the datetime\n",
    "    df['mon'] = pd.to_datetime(df['review_date']).dt.month\n",
    "    df['year'] = pd.to_datetime(df['review_date']).dt.year\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Main function to run the process\n",
    "def main():\n",
    "    # Load data\n",
    "    df = load_data(input_file + input_format)\n",
    "    \n",
    "    # Process data\n",
    "    df_processed = process_data(df)\n",
    "    \n",
    "    # Save processed data\n",
    "    df_processed.to_csv(input_file + '_processed' + '.csv')\n",
    "    # df_processed.to_stata(input_file + '_processed' + '.dta', write_index=False)\n",
    "    \n",
    "    print(\"Data processing complete. Files saved.\")\n",
    "\n",
    "#/Users/ojeongsig/Desktop/yeonguno/did_test/df_review_resnet_merge_processed.csv\n",
    "# /Users/ojeongsig/Desktop/yeonguno/did_test/merged_file_processed.csv\n",
    "#/Users/ojeongsig/Desktop/yeonguno/did_test/df_review_resnet152_merge_resnet2_processed.csv\n",
    "#/Users/ojeongsig/Desktop/yeonguno/did_test/df_review_resnet152_merge_resnet_y_processed.csv\n",
    "#/Users/ojeongsig/Desktop/yeonguno/did_test/df_review_resnet152_merge_resnet_vit_processed.csv\n",
    "#/Users/ojeongsig/Desktop/yeonguno/did_test/df_review_resnet152_merge_resnet_efficient_processed.csv\n",
    "#/Users/ojeongsig/Desktop/yeonguno/did_test/df_review_resnet152_merge_resnet_mpnet_processed.csv\n",
    "#/Users/ojeongsig/Desktop/yeonguno/did_test/resnet_helpful_processed.csv\n",
    "#/Users/ojeongsig/Desktop/yeonguno/resnet_helpful_sentiment_processed.csv \n",
    "#/Users/ojeongsig/Desktop/yeonguno/did_test/resnet_helpful_sentiment_processed2.csv\n",
    "#/Users/ojeongsig/Desktop/yeonguno/did_test/resnet_50_06_14_processed.csv\n",
    "\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('05_30')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "874d36645d7079e1e4c0b4499dcf867001e432ffa87d1300af56b18cb56218d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
