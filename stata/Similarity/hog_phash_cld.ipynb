{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating HOG Similarities: 100%|██████████| 9484/9484 [02:02<00:00, 77.67it/s] \n",
      "Calculating pHash Similarities: 100%|██████████| 9484/9484 [00:31<00:00, 303.36it/s]\n",
      "Calculating CLD Similarities: 100%|██████████| 9484/9484 [01:31<00:00, 103.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete DataFrame saved to 'df_review_hog_phash.csv'.\n",
      "Modified DataFrame saved to 'df_review_hog.dta' without specified columns.\n"
     ]
    }
   ],
   "source": [
    "from imagehash import phash\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from skimage.feature import hog\n",
    "\n",
    "def load_image_from_url_target_size(url, target_size=(128, 128)):\n",
    "    if not url:\n",
    "        return None\n",
    "    try:\n",
    "        resp = requests.get(url)\n",
    "        image = np.asarray(bytearray(resp.content), dtype=\"uint8\")\n",
    "        image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "        # Resize image to target size\n",
    "        image = cv2.resize(image, target_size)\n",
    "        return image\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error loading image from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_first_image_url(image_list_str):\n",
    "    if not image_list_str or image_list_str == '[]':\n",
    "        return None\n",
    "    try:\n",
    "        image_list = json.loads(image_list_str.replace(\"'\", '\"'))\n",
    "        if image_list:\n",
    "            return image_list[0]\n",
    "        else:\n",
    "            return None\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "    \n",
    "def load_image_from_url(url):\n",
    "    if not url:\n",
    "        return None\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        image = Image.open(response.raw).convert('RGB')\n",
    "        return image\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error loading image from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def calculate_hog_similarity(row, target_size=(128, 128)):\n",
    "    if row['cgi_image_url'] is None or row['fgi_image_url'] is None:\n",
    "        return None  # Skip calculation if any URL is missing\n",
    "    image_cgi = load_image_from_url_target_size(row['cgi_image_url'], target_size)\n",
    "    image_fgi = load_image_from_url_target_size(row['fgi_image_url'], target_size)\n",
    "    if image_cgi is None or image_fgi is None:\n",
    "        return None\n",
    "\n",
    "    image_cgi_gray = cv2.cvtColor(image_cgi, cv2.COLOR_BGR2GRAY)\n",
    "    image_fgi_gray = cv2.cvtColor(image_fgi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate HOG features\n",
    "    hog_cgi, _ = hog(image_cgi_gray, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n",
    "    hog_fgi, _ = hog(image_fgi_gray, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n",
    "\n",
    "    # Calculate similarity (cosine similarity)\n",
    "    similarity = np.dot(hog_cgi, hog_fgi) / (np.linalg.norm(hog_cgi) * np.linalg.norm(hog_fgi))\n",
    "    return similarity\n",
    "\n",
    "def calculate_phash_similarity(row):\n",
    "    if row['cgi_image_url'] is None or row['fgi_image_url'] is None:\n",
    "        return None  # Skip calculation if any URL is missing\n",
    "    image_cgi = load_image_from_url(row['cgi_image_url'])\n",
    "    image_fgi = load_image_from_url(row['fgi_image_url'])\n",
    "    if image_cgi is None or image_fgi is None:\n",
    "        return None\n",
    "\n",
    "    hash_cgi = phash(image_cgi)\n",
    "    hash_fgi = phash(image_fgi)\n",
    "\n",
    "    similarity = 1 - (hash_cgi - hash_fgi) / len(hash_cgi.hash) ** 2\n",
    "    return similarity\n",
    "\n",
    "def calculate_cld(image, num_y_coeffs=8, num_cb_coeffs=4, num_cr_coeffs=4):\n",
    "    image = image.resize((64, 64))\n",
    "    ycbcr_image = image.convert('YCbCr')\n",
    "    y, cb, cr = ycbcr_image.split()\n",
    "    y = np.asarray(y, dtype=np.float32)\n",
    "    cb = np.asarray(cb, dtype=np.float32)\n",
    "    cr = np.asarray(cr, dtype=np.float32)\n",
    "\n",
    "    y_dct = cv2.dct(y)\n",
    "    cb_dct = cv2.dct(cb)\n",
    "    cr_dct = cv2.dct(cr)\n",
    "\n",
    "    cld = np.hstack((\n",
    "        y_dct[:num_y_coeffs, :num_y_coeffs].flatten(),\n",
    "        cb_dct[:num_cb_coeffs, :num_cb_coeffs].flatten(),\n",
    "        cr_dct[:num_cr_coeffs, :num_cr_coeffs].flatten()\n",
    "    ))\n",
    "\n",
    "    return cld\n",
    "\n",
    "def calculate_cld_similarity(row):\n",
    "    if row['cgi_image_url'] is None or row['fgi_image_url'] is None:\n",
    "        return None  # Skip calculation if any URL is missing\n",
    "    image_cgi = load_image_from_url(row['cgi_image_url'])\n",
    "    image_fgi = load_image_from_url(row['fgi_image_url'])\n",
    "    if image_cgi is None or image_fgi is None:\n",
    "        return None\n",
    "\n",
    "    cld_cgi = calculate_cld(image_cgi)\n",
    "    cld_fgi = calculate_cld(image_fgi)\n",
    "\n",
    "    similarity = np.dot(cld_cgi, cld_fgi) / (np.linalg.norm(cld_cgi) * np.linalg.norm(cld_fgi))\n",
    "    return similarity\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv('df_review.csv')\n",
    "    df['fgi_image_url'] = df['fgi_images'].apply(extract_first_image_url)\n",
    "    df['cgi_image_url'] = df['cgi_images'].apply(extract_first_image_url)\n",
    "\n",
    "    tqdm.pandas(desc=\"Calculating HOG Similarities\")\n",
    "    df['hog_similarity'] = df.progress_apply(calculate_hog_similarity, axis=1)\n",
    "\n",
    "    tqdm.pandas(desc=\"Calculating pHash Similarities\")\n",
    "    df['phash_similarity'] = df.progress_apply(calculate_phash_similarity, axis=1)\n",
    "\n",
    "    tqdm.pandas(desc=\"Calculating CLD Similarities\")\n",
    "    df['cld_similarity'] = df.progress_apply(calculate_cld_similarity, axis=1)\n",
    "\n",
    "    # Save the complete DataFrame to CSV\n",
    "    df.to_csv('df_review_hog_phash_cld.csv', index=False)\n",
    "    print(\"Complete DataFrame saved to 'df_review_hog_phash.csv'.\")\n",
    "\n",
    "    # Drop the specified columns and save as a Stata DTA file\n",
    "    # columns_to_drop = ['review_text', 'cgi_images', 'fgi_images', 'features']\n",
    "    # df.drop(columns=columns_to_drop, errors='ignore').to_stata('df_review_hog.dta', write_index=False)\n",
    "    print(\"Modified DataFrame saved to 'df_review_hog.dta' without specified columns.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files have been successfully merged and saved to 'merged_file.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vz/_y_gw0w915v4z_x6ps3fjwwm0000gn/T/ipykernel_63730/2882535317.py:28: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  csv2 = csv2.groupby('product_id', group_keys=False).apply(fill_below_first_non_na, columns=columns_to_merge)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the first CSV file\n",
    "csv1 = pd.read_csv('df_review_hog_phash_cld.csv')\n",
    "\n",
    "# Load the second CSV file\n",
    "csv2 = pd.read_csv('df_review.csv')\n",
    "\n",
    "# Set 'Unnamed: 0' as the index for the first DataFrame\n",
    "csv1.set_index('Unnamed: 0', inplace=True)\n",
    "\n",
    "# Select the specific columns to merge from csv1\n",
    "columns_to_merge = ['hog_similarity', 'phash_similarity', 'cld_similarity']\n",
    "\n",
    "# Merge the specified columns into csv2 based on the 'Unnamed: 0' index\n",
    "csv2 = csv2.merge(csv1[columns_to_merge], how='left', left_index=True, right_index=True)\n",
    "\n",
    "# Function to fill values only for rows below the first occurrence of non-missing values\n",
    "def fill_below_first_non_na(df, columns):\n",
    "    for col in columns:\n",
    "        mask = df[col].notna()\n",
    "        first_non_na_idx = mask.idxmax() if mask.any() else None\n",
    "        if first_non_na_idx:\n",
    "            df.loc[first_non_na_idx+1:, col] = df.loc[first_non_na_idx, col]\n",
    "    return df\n",
    "\n",
    "# Apply the function to each group of 'product_id'\n",
    "csv2 = csv2.groupby('product_id', group_keys=False).apply(fill_below_first_non_na, columns=columns_to_merge)\n",
    "\n",
    "# Fill NaN values with 0\n",
    "csv2[columns_to_merge] = csv2[columns_to_merge].fillna(0)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "csv2.to_csv('df_review_hog_phash_cld.csv', index=False)\n",
    "\n",
    "print(\"The files have been successfully merged and saved to 'merged_file.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found.\n",
      "Data processing complete. Files saved.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "input_file = 'df_review_hog_phash_cld'\n",
    "input_format = '.csv'\n",
    "\n",
    "# Function to load data\n",
    "def load_data(filename):\n",
    "    return pd.read_csv(filename)\n",
    "\n",
    "# Function to process data\n",
    "def process_data(df):\n",
    "    # Identify duplicates based on product_id and review_date\n",
    "    duplicates = df[df.duplicated(subset=['product_id', 'review_date'], keep=False)]\n",
    "\n",
    "    # Print duplicates if they exist\n",
    "    if not duplicates.empty:\n",
    "        print(\"Duplicates found:\")\n",
    "        print(duplicates)\n",
    "    else:\n",
    "        print(\"No duplicates found.\")\n",
    "\n",
    "    # Drop duplicates based on product_id and review_date\n",
    "    df = df.drop_duplicates(subset=['product_id', 'review_date'])\n",
    "    \n",
    "    # Convert review_date to the specified datetime format\n",
    "    df['datetime'] = pd.to_datetime(df['review_date']).dt.strftime('%d%b%Y %H:%M:%S')\n",
    "    \n",
    "    # Extract month and year from the datetime\n",
    "    df['mon'] = pd.to_datetime(df['review_date']).dt.month\n",
    "    df['year'] = pd.to_datetime(df['review_date']).dt.year\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Main function to run the process\n",
    "def main():\n",
    "    # Load data\n",
    "    df = load_data(input_file + input_format)\n",
    "    \n",
    "    # Process data\n",
    "    df_processed = process_data(df)\n",
    "    \n",
    "    # Save processed data\n",
    "    df_processed.to_csv(input_file + '.csv', index=False)\n",
    "    # df_processed.to_csv(input_file + '_processed' + '.csv')\n",
    "    # df_processed.to_stata(input_file + '_processed' + '.dta', write_index=False)\n",
    "    \n",
    "    print(\"Data processing complete. Files saved.\")\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
